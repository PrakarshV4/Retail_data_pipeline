{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c33009a9-84b9-4752-8b0c-7f8b686f5463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faker\n",
      "  Using cached faker-37.1.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tzdata in /opt/conda/lib/python3.11/site-packages (from faker) (2023.3)\n",
      "Downloading faker-37.1.0-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faker\n",
      "Successfully installed faker-37.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb4bd5bc-2c2c-4bf1-b2cf-f403cea0b12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40e73db7-08aa-4a49-8fad-8be0074c3689",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/work'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 163\u001b[0m\n\u001b[1;32m    159\u001b[0m     review_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/work/data/raw/fact_reviews.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 163\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/work/data/raw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m     generate_dimensions()\n\u001b[1;32m    165\u001b[0m     generate_facts()\n",
      "File \u001b[0;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "File \u001b[0;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "File \u001b[0;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/work'"
     ]
    }
   ],
   "source": [
    "from faker import Faker\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "# Configuration\n",
    "NUM_CUSTOMERS = 2000\n",
    "NUM_PRODUCTS = 300\n",
    "NUM_TRANSACTIONS = 30000\n",
    "NUM_REVIEWS = 7500\n",
    "START_DATE = datetime(2021, 1, 1)\n",
    "END_DATE = datetime(2023, 1, 1)\n",
    "\n",
    "\n",
    "def generate_dimensions():\n",
    "    # Locations\n",
    "    locations = [(i, fake.city(), fake.state(), fake.country())\n",
    "                 for i in range(1, 501)]\n",
    "    location_df = pd.DataFrame(locations, columns=['location_id', 'city', 'state', 'country'])\n",
    "    location_df.to_csv('data/raw/dim_location.csv', index=False)\n",
    "\n",
    "    # Categories\n",
    "    categories = [\n",
    "        (1, 'Electronics'), (2, 'Clothing'),\n",
    "        (3, 'Home Appliances'), (4, 'Books'),\n",
    "        (5, 'Sports'), (6, 'Beauty'),\n",
    "        (7, 'Toys'), (8, 'Groceries')\n",
    "    ]\n",
    "    category_df = pd.DataFrame(categories, columns=['category_id', 'category_name'])\n",
    "    category_df.to_csv('work/data/raw/dim_category.csv', index=False)\n",
    "\n",
    "    # Products\n",
    "    products = []\n",
    "    for i in range(1, NUM_PRODUCTS + 1):\n",
    "        products.append((\n",
    "            i,\n",
    "            fake.word().title() + \" \" + fake.word().title(),\n",
    "            random.choice(categories)[0],\n",
    "            round(random.uniform(5, 500), 2)\n",
    "        ))\n",
    "\n",
    "    # Introduce NaN values randomly into the `product_name` column\n",
    "    product_df = pd.DataFrame(products, columns=['product_id', 'product_name', 'category_id', 'price'])\n",
    "    product_df['product_name'] = product_df['product_name'].apply(\n",
    "        lambda x: x if random.random() > 0.05 else None\n",
    "    )\n",
    "\n",
    "    # Introduce some duplicate products to simulate data issues\n",
    "    product_df = pd.concat([product_df, product_df.sample(10)], ignore_index=True)\n",
    "\n",
    "    # Introduce an outlier in the price column (e.g., price > 1000)\n",
    "    product_df.loc[product_df.sample(5).index, 'price'] = random.randint(1000, 5000)\n",
    "\n",
    "    product_df.to_csv('/work/data/raw/dim_product.csv', index=False)\n",
    "\n",
    "    # Customers\n",
    "    customers = []\n",
    "    for i in range(1, NUM_CUSTOMERS + 1):\n",
    "        customers.append((\n",
    "            i,\n",
    "            fake.name(),\n",
    "            fake.email(),\n",
    "            fake.date_between(START_DATE, END_DATE),\n",
    "            random.choice(locations)[0]\n",
    "        ))\n",
    "\n",
    "    customer_df = pd.DataFrame(customers, columns=['customer_id', 'name', 'email', 'signup_date', 'location_id'])\n",
    "\n",
    "    # Introduce NaN values randomly into the `email` and `signup_date` columns\n",
    "    customer_df['email'] = customer_df['email'].apply(\n",
    "        lambda x: x if random.random() > 0.05 else None\n",
    "    )\n",
    "    customer_df['signup_date'] = customer_df['signup_date'].apply(\n",
    "        lambda x: x if random.random() > 0.05 else None\n",
    "    )\n",
    "\n",
    "    # Introduce duplicate customers to simulate data issues\n",
    "    customer_df = pd.concat([customer_df, customer_df.sample(5)], ignore_index=True)\n",
    "\n",
    "    customer_df.to_csv('/work/data/raw/dim_customer.csv', index=False)\n",
    "def generate_facts():\n",
    "    # Date dimension\n",
    "    dates = []\n",
    "    current_date = START_DATE\n",
    "    while current_date <= END_DATE:\n",
    "        dates.append((\n",
    "            current_date.date(),\n",
    "            current_date.day,\n",
    "            current_date.month,\n",
    "            (current_date.month - 1) // 3 + 1,\n",
    "            current_date.year,\n",
    "            current_date.weekday() + 1,\n",
    "            current_date.weekday() >= 5\n",
    "        ))\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    date_df = pd.DataFrame(dates, columns=['date_id', 'day', 'month', 'quarter', 'year', 'day_of_week', 'is_weekend'])\n",
    "    date_df.to_csv('/work/data/raw/dim_date.csv', index=False)\n",
    "\n",
    "    # Transactions\n",
    "    transactions = []\n",
    "    products = pd.read_csv('/work/data/raw/dim_product.csv')\n",
    "    for i in range(1, NUM_TRANSACTIONS + 1):\n",
    "        product = products.sample(1).iloc[0]\n",
    "        transactions.append((\n",
    "            i,\n",
    "            random.randint(1, NUM_CUSTOMERS),\n",
    "            product['product_id'],\n",
    "            fake.date_between(START_DATE, END_DATE).isoformat(),\n",
    "            round(product['price'] * random.uniform(0.8, 1.2) * random.randint(1, 5), 2),\n",
    "            random.randint(1, 5)\n",
    "        ))\n",
    "\n",
    "    transaction_df = pd.DataFrame(transactions,\n",
    "                                  columns=['transaction_id', 'customer_id', 'product_id', 'date_id', 'amount',\n",
    "                                           'quantity'])\n",
    "\n",
    "    # Introduce NaN values randomly into the `amount` column\n",
    "    transaction_df['amount'] = transaction_df['amount'].apply(\n",
    "        lambda x: x if random.random() > 0.05 else None\n",
    "    )\n",
    "\n",
    "    # Introduce duplicate transactions to simulate data issues\n",
    "    transaction_df = pd.concat([transaction_df, transaction_df.sample(10)], ignore_index=True)\n",
    "\n",
    "    # Introduce outliers in the `amount` column (e.g., extremely high values)\n",
    "    transaction_df.loc[transaction_df.sample(5).index, 'amount'] = random.randint(1000, 10000)\n",
    "\n",
    "    transaction_df.to_csv('/work/data/raw/fact_transactions.csv', index=False)\n",
    "\n",
    "    # Reviews\n",
    "    reviews = []\n",
    "    for i in range(1, NUM_REVIEWS + 1):\n",
    "        transaction = random.choice(transactions)\n",
    "        reviews.append((\n",
    "            i,\n",
    "            transaction[1],  # customer_id\n",
    "            transaction[2],  # product_id\n",
    "            (datetime.strptime(transaction[3], '%Y-%m-%d') + timedelta(days=random.randint(1, 30))).date(),\n",
    "            random.randint(1, 5),\n",
    "            fake.paragraph()\n",
    "        ))\n",
    "\n",
    "    review_df = pd.DataFrame(reviews,\n",
    "                             columns=['review_id', 'customer_id', 'product_id', 'date_id', 'rating', 'review_text'])\n",
    "\n",
    "    # Introduce NaN values randomly into the `review_text` column\n",
    "    review_df['review_text'] = review_df['review_text'].apply(\n",
    "        lambda x: x if random.random() > 0.05 else None\n",
    "    )\n",
    "\n",
    "    # Introduce duplicate reviews to simulate data issues\n",
    "    review_df = pd.concat([review_df, review_df.sample(5)], ignore_index=True)\n",
    "\n",
    "    review_df.to_csv('/work/data/raw/fact_reviews.csv', index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs('/work/data/raw', exist_ok=True)\n",
    "    generate_dimensions()\n",
    "    generate_facts()\n",
    "    print(\"Data generation complete! Files saved to data/raw/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a487b190-83a8-4797-ad26-a8b78e187319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Data for dim_category:\n",
      "+-----------+-------------+\n",
      "|category_id|category_name|\n",
      "+-----------+-------------+\n",
      "|          8|    Groceries|\n",
      "|          2|     Clothing|\n",
      "|          7|         Toys|\n",
      "|          6|       Beauty|\n",
      "|          4|        Books|\n",
      "+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Cleaned Data for dim_customer:\n",
      "+-----------+-----------------+--------------------+-----------+-----------+\n",
      "|customer_id|             name|               email|signup_date|location_id|\n",
      "+-----------+-----------------+--------------------+-----------+-----------+\n",
      "|        160|   Jennifer Riggs|sellersjonathan@e...| 2022-05-08|        448|\n",
      "|        413|Victoria Gonzalez|kellerjoy@example...| 2022-07-02|        139|\n",
      "|        452|    James Freeman|mayertonya@exampl...| 2021-07-27|        261|\n",
      "|        940|        Erik Dunn|banksjohn@example...| 2022-04-01|        202|\n",
      "|        980|  Christopher Lam|josephlarson@exam...| 2021-03-15|        107|\n",
      "+-----------+-----------------+--------------------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Cleaned Data for dim_date:\n",
      "+----------+---+-----+-------+----+-----------+----------+\n",
      "|   date_id|day|month|quarter|year|day_of_week|is_weekend|\n",
      "+----------+---+-----+-------+----+-----------+----------+\n",
      "|2021-12-08|  8|   12|      4|2021|          3|     false|\n",
      "|2022-04-28| 28|    4|      2|2022|          4|     false|\n",
      "|2022-06-27| 27|    6|      2|2022|          1|     false|\n",
      "|2022-06-25| 25|    6|      2|2022|          6|      true|\n",
      "|2022-08-09|  9|    8|      3|2022|          2|     false|\n",
      "+----------+---+-----+-------+----+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Cleaned Data for dim_location:\n",
      "+-----------+-----------------+--------------+-------+\n",
      "|location_id|             city|         state|country|\n",
      "+-----------+-----------------+--------------+-------+\n",
      "|         63|        Terrytown|      New York|  Congo|\n",
      "|        311|       East Kayla|      Colorado|Vietnam|\n",
      "|        426|      Lake Robert| Massachusetts|Tokelau|\n",
      "|         30|Lake Breannamouth|       Arizona|Iceland|\n",
      "|        271|        Scottside|North Carolina| Canada|\n",
      "+-----------+-----------------+--------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Cleaned Data for dim_product:\n",
      "+----------+-------------+-----------+------+\n",
      "|product_id| product_name|category_id| price|\n",
      "+----------+-------------+-----------+------+\n",
      "|        27|   Into Build|          8|181.58|\n",
      "|       108|   Phone Bill|          2|302.19|\n",
      "|       142|Identify Seek|          2| 95.61|\n",
      "|       122|   In Station|          6|146.45|\n",
      "|       195|Record Source|          1|326.27|\n",
      "+----------+-------------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Cleaned Data for fact_reviews:\n",
      "+---------+-----------+----------+----------+------+--------------------+\n",
      "|review_id|customer_id|product_id|   date_id|rating|         review_text|\n",
      "+---------+-----------+----------+----------+------+--------------------+\n",
      "|       51|       1292|       280|2022-06-21|     3|Argue for souther...|\n",
      "|      220|        397|        67|2022-05-08|     1|Paper discover ar...|\n",
      "|      578|        905|       241|2021-02-27|     2|Local could kitch...|\n",
      "|      684|        954|       196|2021-02-10|     3|Front doctor fede...|\n",
      "|      970|       1638|        10|2022-03-25|     5|Process continue ...|\n",
      "+---------+-----------+----------+----------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Cleaned Data for fact_transaction:\n",
      "+--------------+-----------+----------+----------+------+--------+\n",
      "|transaction_id|customer_id|product_id|   date_id|amount|quantity|\n",
      "+--------------+-----------+----------+----------+------+--------+\n",
      "|           222|        507|       240|2022-04-03|417.64|       3|\n",
      "|           263|         66|       142|2022-08-14| 113.2|       3|\n",
      "|           325|        598|       113|2022-08-22|348.31|       3|\n",
      "|           448|        604|       201|2021-05-02| 19.97|       2|\n",
      "|          1271|       1833|       238|2021-12-23|760.47|       4|\n",
      "+--------------+-----------+----------+----------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, mean, stddev\n",
    "from pyspark.sql.types import FloatType, DoubleType, IntegerType\n",
    "\n",
    "# 1️⃣ Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"Data_Cleaning\").getOrCreate()\n",
    "\n",
    "# 2️⃣ List of CSV files\n",
    "files = {\n",
    "    \"dim_category\": \"data/raw/dim_category.csv\",\n",
    "    \"dim_customer\": \"data/raw/dim_customer.csv\",\n",
    "    \"dim_date\": \"data/raw/dim_date.csv\",\n",
    "    \"dim_location\": \"data/raw/dim_location.csv\",\n",
    "    \"dim_product\": \"data/raw/dim_product.csv\",\n",
    "    \"fact_reviews\": \"data/raw/fact_reviews.csv\",\n",
    "    \"fact_transaction\": \"data/raw/fact_transactions.csv\"\n",
    "}\n",
    "\n",
    "# 3️⃣ Load CSV Files\n",
    "dataframes = {name: spark.read.csv(path, header=True, inferSchema=True) for name, path in files.items()}\n",
    "\n",
    "# 4️⃣ Data Cleaning Functions\n",
    "def clean_dataframe(df):\n",
    "    # Remove rows with nulls in any column\n",
    "    df = df.dropna(how='any')\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df = df.dropDuplicates()\n",
    "    \n",
    "    # Remove outliers (Z-score method for numeric columns)\n",
    "    numeric_cols = [field.name for field in df.schema.fields if isinstance(field.dataType, (FloatType, DoubleType, IntegerType))]\n",
    "    for col_name in numeric_cols:\n",
    "        mean_val = df.select(mean(col(col_name))).collect()[0][0]\n",
    "        stddev_val = df.select(stddev(col(col_name))).collect()[0][0]\n",
    "        \n",
    "        if mean_val is not None and stddev_val is not None and stddev_val != 0:\n",
    "            df = df.filter((col(col_name) - mean_val) / stddev_val < 3)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 5️⃣ Apply Cleaning Function\n",
    "cleaned_dataframes = {name: clean_dataframe(df) for name, df in dataframes.items()}\n",
    "\n",
    "# 6️⃣ Save Cleaned Data (optional)\n",
    "for name, df in cleaned_dataframes.items():\n",
    "    df.write.mode('overwrite').csv(f\"data/refined/cleaned_{name}.csv\", header=True)\n",
    "\n",
    "# 7️⃣ Show Cleaned Data (optional)\n",
    "for name, df in cleaned_dataframes.items():\n",
    "    print(f\"Cleaned Data for {name}:\")\n",
    "    df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4194241-ad8a-45ad-8bac-ac23ced2ab69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sqlalchemy in /opt/conda/lib/python3.11/site-packages (2.0.22)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy) (4.8.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy) (3.0.0)\n"
     ]
    }
   ],
   "source": [
    "# Adding refined data into postgress\n",
    "!pip install sqlalchemy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9cca365-2123-4966-93e0-a28a6054182a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (4.9 kB)\n",
      "Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.9.10\n"
     ]
    }
   ],
   "source": [
    "!pip install psycopg2-binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9873496b-f972-477a-a505-5014b9ef69ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully connected to PostgreSQL!\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Define PostgreSQL credentials\n",
    "POSTGRES_USER = \"postgres\"\n",
    "POSTGRES_PASSWORD = \"password\"  # Replace with actual password\n",
    "POSTGRES_DB = \"myapp\"  # Replace with actual database name\n",
    "POSTGRES_HOST = \"db\"  # Container name\n",
    "POSTGRES_PORT = \"5432\"\n",
    "\n",
    "# Create SQLAlchemy connection string\n",
    "DATABASE_URL = f\"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}\"\n",
    "\n",
    "# Establish connection\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        print(\"✅ Successfully connected to PostgreSQL!\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Connection failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9de2ff4c-7eab-4580-ba53-887cfe96c1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dim_category to PostgreSQL\n",
      "Saved dim_customer to PostgreSQL\n",
      "Saved dim_date to PostgreSQL\n",
      "Saved dim_location to PostgreSQL\n",
      "Saved dim_product to PostgreSQL\n",
      "Saved fact_reviews to PostgreSQL\n",
      "Saved fact_transaction to PostgreSQL\n"
     ]
    }
   ],
   "source": [
    "# Loading data into postgress\n",
    "\n",
    "df_category = spark.read.csv(\"/home/jovyan/data/refined/cleaned_dim_category.csv\", header=True, inferSchema=True)\n",
    "df_customer = spark.read.csv(\"/home/jovyan/data/refined/cleaned_dim_customer.csv\", header=True, inferSchema=True)\n",
    "df_date = spark.read.csv(\"/home/jovyan/data/refined/cleaned_dim_date.csv\", header=True, inferSchema=True)\n",
    "df_location = spark.read.csv(\"/home/jovyan/data/refined/cleaned_dim_location.csv\", header=True, inferSchema=True)\n",
    "df_product = spark.read.csv(\"/home/jovyan/data/refined/cleaned_dim_product.csv\", header=True, inferSchema=True)\n",
    "df_fact_reviews = spark.read.csv(\"/home/jovyan/data/refined/cleaned_fact_reviews.csv\", header=True, inferSchema=True)\n",
    "df_fact_transaction = spark.read.csv(\"/home/jovyan/data/refined/cleaned_fact_transaction.csv\", header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "def save_to_postgres(spark_df, table_name, engine):\n",
    "    # Convert PySpark DataFrame to Pandas DataFrame\n",
    "    pandas_df = spark_df.toPandas()\n",
    "    \n",
    "    # Save to PostgreSQL\n",
    "    pandas_df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "    print(f\"Saved {table_name} to PostgreSQL\")\n",
    "\n",
    "\n",
    "save_to_postgres(df_category, \"dim_category\", engine)\n",
    "save_to_postgres(df_customer, \"dim_customer\", engine)\n",
    "save_to_postgres(df_date, \"dim_date\", engine)\n",
    "save_to_postgres(df_location, \"dim_location\", engine)\n",
    "save_to_postgres(df_product, \"dim_product\", engine)\n",
    "save_to_postgres(df_fact_reviews, \"fact_reviews\", engine)\n",
    "save_to_postgres(df_fact_transaction, \"fact_transaction\", engine)\n",
    "# # Store in PostgreSQL table\n",
    "# df.to_sql(\"category\", con=engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "# print(\"✅ Data loaded successfully into PostgreSQL!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b032e131-266b-47af-a325-972c5dca019c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>name</th>\n",
       "      <th>email</th>\n",
       "      <th>signup_date</th>\n",
       "      <th>location_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160</td>\n",
       "      <td>Jennifer Riggs</td>\n",
       "      <td>sellersjonathan@example.org</td>\n",
       "      <td>2022-05-08</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>413</td>\n",
       "      <td>Victoria Gonzalez</td>\n",
       "      <td>kellerjoy@example.com</td>\n",
       "      <td>2022-07-02</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>452</td>\n",
       "      <td>James Freeman</td>\n",
       "      <td>mayertonya@example.org</td>\n",
       "      <td>2021-07-27</td>\n",
       "      <td>261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>940</td>\n",
       "      <td>Erik Dunn</td>\n",
       "      <td>banksjohn@example.org</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>980</td>\n",
       "      <td>Christopher Lam</td>\n",
       "      <td>josephlarson@example.com</td>\n",
       "      <td>2021-03-15</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id               name                        email signup_date  \\\n",
       "0          160     Jennifer Riggs  sellersjonathan@example.org  2022-05-08   \n",
       "1          413  Victoria Gonzalez        kellerjoy@example.com  2022-07-02   \n",
       "2          452      James Freeman       mayertonya@example.org  2021-07-27   \n",
       "3          940          Erik Dunn        banksjohn@example.org  2022-04-01   \n",
       "4          980    Christopher Lam     josephlarson@example.com  2021-03-15   \n",
       "\n",
       "   location_id  \n",
       "0          448  \n",
       "1          139  \n",
       "2          261  \n",
       "3          202  \n",
       "4          107  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_sql(\"SELECT * FROM dim_customer;\", engine)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ec2518-9edf-420b-9adb-177a9fee214e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
